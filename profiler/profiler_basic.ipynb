{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGZcVHv6s94b"
      },
      "source": [
        "# Basics - Pytorch Profiler\n",
        "\n",
        "Best to run on colab if you are on a non CUDA device locally like mps.\n",
        "\n",
        "Sources\n",
        "- https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mtiebvk-s94c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6DUrvVF9s94d"
      },
      "outputs": [],
      "source": [
        "# Simple resnet\n",
        "model = models.resnet18()\n",
        "\n",
        "# Example input\n",
        "inputs = torch.randn(5, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKqJXHaJs94e"
      },
      "source": [
        "PyTorch profiler is enabled through the context manager and accepts\n",
        "a number of parameters, some of the most useful are:\n",
        "\n",
        "- ``activities`` - a list of activities to profile:\n",
        "   - ``ProfilerActivity.CPU`` - PyTorch operators, TorchScript functions and\n",
        "     user-defined code labels (see ``record_function`` below);\n",
        "   - ``ProfilerActivity.CUDA`` - on-device CUDA kernels;\n",
        "   - Unfortunately they don't seem to have an MPS one (yet) as of 7dec2022\n",
        "- ``record_shapes`` - whether to record shapes of the operator inputs;\n",
        "- ``profile_memory`` - whether to report amount of memory consumed by\n",
        "  model's Tensors;\n",
        "- ``use_cuda`` - whether to measure execution time of CUDA kernels.\n",
        "\n",
        "Note: when using CUDA, profiler also shows the runtime CUDA events\n",
        "occuring on the host."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "raifYWEAtUM6"
      },
      "outputs": [],
      "source": [
        "# Start profiling the CPU\n",
        "# The record_function context manager a 'code range' that is being tracked\n",
        "# You can create multiple of these 'ranges' which will be tracked in parralel\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "\n",
        "    # record_function context manager to label arbitrary code ranges with user provided name\n",
        "    # This means everything within that context is tracked as part of the name we give it 'model_inference'\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0piX07FVu3o2"
      },
      "source": [
        "Note that we can use ``record_function`` context manager to label\n",
        "arbitrary code ranges with user provided names\n",
        "(``model_inference`` is used as a label in the example above).\n",
        "\n",
        "Profiler allows one to check which operators were called during the\n",
        "execution of a code range wrapped with a profiler context manager.\n",
        "If multiple profiler ranges are active at the same time (e.g. in\n",
        "parallel PyTorch threads), each profiling context manager tracks only\n",
        "the operators of its corresponding range.\n",
        "Profiler also automatically profiles the async tasks launched\n",
        "with ``torch.jit._fork`` and (in case of a backward pass)\n",
        "the backward pass operators launched with ``backward()`` call.\n",
        "\n",
        "Let's print out the stats for the execution above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTFjUySku7xu",
        "outputId": "637410e6-dc40-426d-9dbd-1caa587ea7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         8.23%      17.104ms        99.55%     206.778ms     206.778ms             1  \n",
            "                     aten::conv2d         0.13%     260.000us        60.17%     124.992ms       6.250ms            20  \n",
            "                aten::convolution         1.28%       2.662ms        60.05%     124.732ms       6.237ms            20  \n",
            "               aten::_convolution         1.45%       3.012ms        58.77%     122.070ms       6.104ms            20  \n",
            "         aten::mkldnn_convolution        57.18%     118.770ms        57.32%     119.058ms       5.953ms            20  \n",
            "                 aten::batch_norm         0.04%      80.000us         7.58%      15.752ms     787.600us            20  \n",
            "     aten::_batch_norm_impl_index         1.29%       2.680ms         7.54%      15.672ms     783.600us            20  \n",
            "        aten::adaptive_avg_pool2d         0.45%     933.000us         6.79%      14.107ms      14.107ms             1  \n",
            "                       aten::mean         0.83%       1.729ms         6.34%      13.174ms      13.174ms             1  \n",
            "          aten::native_batch_norm         6.14%      12.754ms         6.23%      12.950ms     647.500us            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 207.716ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruNfOF0ivXHf"
      },
      "source": [
        "### Reading the profiled results\n",
        "* Difference between ``Self CPU .. `` and ``CPU ..``\n",
        "    * Operators in the model can call other 'child' operators. \n",
        "    * CPU: Includes the time spend in children operations\n",
        "    * Self CPU: Excludes those, so only time spend on that specifci operator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynra6tbYwzJ6"
      },
      "source": [
        "Here we see that, as expected, most of the time is spent in convolution (and specifically in ``mkldnn_convolution``\n",
        "for PyTorch compiled with MKL-DNN support).\n",
        "Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time excludes time\n",
        "spent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing\n",
        "``sort_by=\"self_cpu_time_total\"`` into the ``table`` call.\n",
        "\n",
        "To get a finer granularity of results and include operator input shapes, pass ``group_by_input_shape=True``\n",
        "(note: this requires running the profiler with ``record_shapes=True``):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyPXxOxVu_1-",
        "outputId": "b555f37f-42d5-4278-9fac-0372e17ea6ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "                  model_inference         8.23%      17.104ms        99.55%     206.778ms     206.778ms             1                                                                                []  \n",
            "                     aten::conv2d         0.09%     184.000us        34.51%      71.673ms      71.673ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
            "                aten::convolution         1.16%       2.418ms        34.42%      71.489ms      71.489ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n",
            "               aten::_convolution         1.37%       2.849ms        33.25%      69.071ms      69.071ms             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]  \n",
            "         aten::mkldnn_convolution        31.82%      66.094ms        31.88%      66.222ms      66.222ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
            "                     aten::conv2d         0.01%      20.000us         6.85%      14.222ms       3.555ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
            "                aten::convolution         0.03%      58.000us         6.84%      14.202ms       3.550ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
            "               aten::_convolution         0.02%      46.000us         6.81%      14.144ms       3.536ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
            "        aten::adaptive_avg_pool2d         0.45%     933.000us         6.79%      14.107ms      14.107ms             1                                                              [[5, 512, 7, 7], []]  \n",
            "         aten::mkldnn_convolution         6.77%      14.054ms         6.79%      14.098ms       3.525ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "Self CPU time total: 207.716ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au_u4bNwxJLc"
      },
      "source": [
        "Note the occurence of ``aten::convolution`` twice with different input shapes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD9zFgiHxPPm"
      },
      "source": [
        "### Now let's try to analyze GPU usage\n",
        "\n",
        "Do note that we are also profiling CPU usage, so the output table is now relatively large and you'll need to scroll to the right to see the actual CUDA usage. \n",
        "\n",
        "Both are tracked, as some operations are performed on the GPU, while some do still need to happen on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2JiLFZTw4uq",
        "outputId": "3ca31ed6-709a-4df2-dc10-e1e7aba195fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        model_inference         0.05%       3.196ms       100.00%        6.884s        6.884s       0.000us         0.00%       1.752ms       1.752ms             1  \n",
            "                                           aten::conv2d         0.00%     115.000us        94.50%        6.505s     325.275ms       0.000us         0.00%       1.148ms      57.400us            20  \n",
            "                                      aten::convolution         0.01%     471.000us        94.49%        6.505s     325.269ms       0.000us         0.00%       1.148ms      57.400us            20  \n",
            "                                     aten::_convolution         0.01%     459.000us        94.49%        6.505s     325.245ms       0.000us         0.00%       1.148ms      57.400us            20  \n",
            "                                aten::cudnn_convolution         9.06%     623.755ms        94.48%        6.504s     325.222ms       1.148ms        65.53%       1.148ms      57.400us            20  \n",
            "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     327.000us        18.66%     327.000us       9.618us            34  \n",
            "                                       aten::batch_norm         0.00%      65.000us         4.51%     310.439ms      15.522ms       0.000us         0.00%     301.000us      15.050us            20  \n",
            "                           aten::_batch_norm_impl_index         0.02%       1.188ms         4.51%     310.374ms      15.519ms       0.000us         0.00%     301.000us      15.050us            20  \n",
            "                                 aten::cudnn_batch_norm         0.24%      16.598ms         4.49%     309.186ms      15.459ms     301.000us        17.18%     301.000us      15.050us            20  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     172.000us         9.82%     172.000us      11.467us            15  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 6.884s\n",
            "Self CUDA time total: 1.752ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[\n",
        "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgt-5YXxxeJA"
      },
      "source": [
        "Note in my notebook: Note the occurence of on-device kernels in the output (e.g. ``sgemm_32x32x32_NN``).\n",
        "\n",
        "A few things to note:\n",
        "* CPU times seem to be really low, why is that?\n",
        "    - Because most operations are now performed on CUDA, so the CPU does nothing for those operations\n",
        "    - Please scroll to the right to see the actual CUDA output.\n",
        "* What we do see is that ``aten:mkldnn_convolution`` has been changed in this model for ``aten:cudnn_convolution``. Which also takes most of the CUDA memory in this case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRtX04d5zKZz"
      },
      "source": [
        "### Using profiler to analyze memory consumption\n",
        "\n",
        "PyTorch profiler can also show the amount of memory (used by the model's tensors)\n",
        "that was allocated (or released) during the execution of the model's operators.\n",
        "In the output below, 'self' memory corresponds to the memory allocated (released)\n",
        "by the operator, excluding the children calls to the other operators.\n",
        "To enable memory profiling functionality pass ``profile_memory=True``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2JGhL8DxVtB",
        "outputId": "af50f136-92c0-45a1-ecde-d973577bff3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.54%     503.000us         0.54%     503.000us       2.515us      94.86 Mb      94.86 Mb           200  \n",
            "    aten::max_pool2d_with_indices        10.54%       9.837ms        10.54%       9.837ms       9.837ms      11.48 Mb      11.48 Mb             1  \n",
            "                      aten::addmm         0.35%     324.000us         0.36%     337.000us     337.000us      19.53 Kb      19.53 Kb             1  \n",
            "                       aten::mean         0.04%      39.000us         0.15%     143.000us     143.000us      10.00 Kb      10.00 Kb             1  \n",
            "              aten::empty_strided         0.00%       4.000us         0.00%       4.000us       4.000us           4 b           4 b             1  \n",
            "                     aten::conv2d         0.11%     101.000us        74.66%      69.660ms       3.483ms      47.37 Mb           0 b            20  \n",
            "                aten::convolution         0.42%     394.000us        74.55%      69.559ms       3.478ms      47.37 Mb           0 b            20  \n",
            "               aten::_convolution         0.25%     235.000us        74.13%      69.165ms       3.458ms      47.37 Mb           0 b            20  \n",
            "         aten::mkldnn_convolution        73.55%      68.620ms        73.88%      68.930ms       3.446ms      47.37 Mb           0 b            20  \n",
            "                aten::as_strided_         0.04%      40.000us         0.04%      40.000us       2.000us           0 b           0 b            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 93.301ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18()\n",
        "inputs = torch.randn(5, 3, 224, 224)\n",
        "\n",
        "# profile_memory=True needs to be added to be able to analyze/profile memory usage\n",
        "with profile(activities=[ProfilerActivity.CPU],\n",
        "        profile_memory=True, record_shapes=True) as prof:\n",
        "    model(inputs)\n",
        "\n",
        "# Please scroll to the right to see memory usage\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Cl_0JJzyvz"
      },
      "source": [
        "What do we see:\n",
        "* As a reminder the ``Self``, means that any child operations are excluded from the total for that operation in the network\n",
        "* We see that all the convolution layers don't seem to take any memory THEMSELVES\n",
        "\n",
        "Now let's try to sort of the normal CPU memory, meaning the child operations are also included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo12qCLazT6Q",
        "outputId": "5a15bf02-74d8-406d-d34f-0220c8a61643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.54%     503.000us         0.54%     503.000us       2.515us      94.86 Mb      94.86 Mb           200  \n",
            "                 aten::batch_norm         0.09%      80.000us        12.24%      11.417ms     570.850us      47.41 Mb           0 b            20  \n",
            "     aten::_batch_norm_impl_index         0.16%     147.000us        12.15%      11.337ms     566.850us      47.41 Mb           0 b            20  \n",
            "          aten::native_batch_norm        11.66%      10.875ms        11.95%      11.152ms     557.600us      47.41 Mb     -75.00 Kb            20  \n",
            "                     aten::conv2d         0.11%     101.000us        74.66%      69.660ms       3.483ms      47.37 Mb           0 b            20  \n",
            "                aten::convolution         0.42%     394.000us        74.55%      69.559ms       3.478ms      47.37 Mb           0 b            20  \n",
            "               aten::_convolution         0.25%     235.000us        74.13%      69.165ms       3.458ms      47.37 Mb           0 b            20  \n",
            "         aten::mkldnn_convolution        73.55%      68.620ms        73.88%      68.930ms       3.446ms      47.37 Mb           0 b            20  \n",
            "                 aten::empty_like         0.09%      82.000us         0.14%     135.000us       6.750us      47.37 Mb           0 b            20  \n",
            "                 aten::max_pool2d         0.01%      11.000us        10.56%       9.848ms       9.848ms      11.48 Mb           0 b             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 93.301ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vJCMnZq0Ou1"
      },
      "source": [
        "What do we see:\n",
        "* We see that the memory taken seems to stay about the same untill the ``atten:empty`` layer.\n",
        "* This can indicate that the child the process of each operations do keep and move over some memory between layers which doesn't seem to be released that much as most layers have the same amount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St_689na01ID"
      },
      "source": [
        "### Using tracing functionality\n",
        "\n",
        "Profiling results can be outputted as a .json trace file. After you run this, please download the trace.json file and then go to [chrome://tracing](chrome://tracing) and upload the file to see the visualization from it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vA_ufVhKziuS"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "    model(inputs)\n",
        "\n",
        "prof.export_chrome_trace(\"trace.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zup7fo1-Vq"
      },
      "source": [
        "What do we see:\n",
        "* You see all the operations that happen in parralelel on a differrent row.\n",
        "* For each operation you can click on it and inspect what operation happened, there like a simple add_ or perhaps a more complicated attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li3kJTt518zz"
      },
      "source": [
        "### Examining stack traces\n",
        "\n",
        "Profiler can be used to analyze Python and TorchScript stack traces.\n",
        "\n",
        "Basically you see which files and functions/methods in those files are triggered for that specific operation.\n",
        "\n",
        "This can be very usefull to figure out where a function or file is located that clocks up you model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAOAh-kp036v",
        "outputId": "ba73e6ae-ad91-4a47-b391-48c7454e9ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Source Location                                            \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
            "                                aten::cudnn_convolution        41.66%       4.168ms        53.21%       5.324ms     266.200us       1.136ms        65.48%       1.136ms      56.800us            20  runpy.py(87): _run_code                                    \n",
            "                                                                                                                                                                                                     ipykernel_launcher.py(16): <module>                        \n",
            "                                                                                                                                                                                                     traitlets/config/application.py(846): launch_instance      \n",
            "                                                                                                                                                                                                     ipykernel/kernelapp.py(612): start                         \n",
            "                                                                                                                                                                                                     tornado/platform/asyncio.py(149): start                    \n",
            "                                                                                                                                                                                                                                                                \n",
            "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     327.000us        18.85%     327.000us       9.618us            34                                                             \n",
            "                                                                                                                                                                                                                                                                \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
            "Self CPU time total: 10.006ms\n",
            "Self CUDA time total: 1.735ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    with_stack=True,\n",
        ") as prof:\n",
        "    model(inputs)\n",
        "\n",
        "# Print aggregated stats\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc12Yq592nB2"
      },
      "source": [
        "Note the two convolutions and the two callsites in ``torchvision/models/resnet.py`` script.\n",
        "\n",
        "(Warning: stack tracing adds an extra profiling overhead.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C8shPzi2y1Y"
      },
      "source": [
        "### Visualizing data as a flamegraph\n",
        "\n",
        "Execution time (``self_cpu_time_total`` and ``self_cuda_time_total`` metrics) and stack traces\n",
        "can also be visualized as a flame graph. To do this, first export the raw data using ``export_stacks`` (requires ``with_stack=True``):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "787k7RX-2OqL"
      },
      "outputs": [],
      "source": [
        "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXraRiUF23wf",
        "outputId": "e4ce444b-2a53-43a9-d8db-5dacdd140bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'FlameGraph'...\n",
            "remote: Enumerating objects: 1217, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 1217 (delta 43), reused 59 (delta 32), pack-reused 1136\u001b[K\n",
            "Receiving objects: 100% (1217/1217), 1.93 MiB | 907.00 KiB/s, done.\n",
            "Resolving deltas: 100% (700/700), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/brendangregg/FlameGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ScrSFWAu27fl"
      },
      "outputs": [],
      "source": [
        "!cd FlameGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMRlMsVf29yS",
        "outputId": "87364fdd-ac94-403e-e07b-1afbc75fd6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: ./flamegraph.pl: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt > perf_viz.svg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf3lBn2I3CJI"
      },
      "source": [
        "### Using profiler to analyze long-running jobs\n",
        "\n",
        "PyTorch profiler offers an additional API to handle long-running jobs\n",
        "(such as training loops). Tracing all of the execution can be\n",
        "slow and result in very large trace files. To avoid this, use optional\n",
        "arguments:\n",
        "\n",
        "- ``schedule`` - specifies a function that takes an integer argument (step number)\n",
        "  as an input and returns an action for the profiler, the best way to use this parameter\n",
        "  is to use ``torch.profiler.schedule`` helper function that can generate a schedule for you;\n",
        "- ``on_trace_ready`` - specifies a function that takes a reference to the profiler as\n",
        "  an input and is called by the profiler each time the new trace is ready.\n",
        "\n",
        "To illustrate how the API works, let's first consider the following example with\n",
        "``torch.profiler.schedule`` helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wTkGrnTf3AIT"
      },
      "outputs": [],
      "source": [
        "from torch.profiler import schedule\n",
        "\n",
        "my_schedule = schedule(\n",
        "    skip_first=10,\n",
        "    wait=5,\n",
        "    warmup=1,\n",
        "    active=3,\n",
        "    repeat=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IqtePQW3OFK"
      },
      "source": [
        "Profiler assumes that the long-running job is composed of steps, numbered\n",
        "starting from zero. The example above defines the following sequence of actions\n",
        "for the profiler:\n",
        "\n",
        "1. Parameter ``skip_first`` tells profiler that it should ignore the first 10 steps\n",
        "   (default value of ``skip_first`` is zero);\n",
        "2. After the first ``skip_first`` steps, profiler starts executing profiler cycles;\n",
        "3. Each cycle consists of three phases:\n",
        "\n",
        "   - idling (``wait=5`` steps), during this phase profiler is not active;\n",
        "   - warming up (``warmup=1`` steps), during this phase profiler starts tracing, but\n",
        "     the results are discarded; this phase is used to discard the samples obtained by\n",
        "     the profiler at the beginning of the trace since they are usually skewed by an extra\n",
        "     overhead;\n",
        "   - active tracing (``active=3`` steps), during this phase profiler traces and records data;\n",
        "4. An optional ``repeat`` parameter specifies an upper bound on the number of cycles.\n",
        "   By default (zero value), profiler will execute cycles as long as the job runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8doUGhD3buv"
      },
      "source": [
        "Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,\n",
        "actively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively\n",
        "record another 3 steps. Since the ``repeat=2`` parameter value is specified, the profiler will stop\n",
        "the recording after the first two cycles.\n",
        "\n",
        "At the end of each cycle profiler calls the specified ``on_trace_ready`` function and passes itself as\n",
        "an argument. This function is used to process the new trace - either by obtaining the table output or\n",
        "by saving the output on disk as a trace file.\n",
        "\n",
        "To send the signal to the profiler that the next step has started, call ``prof.step()`` function.\n",
        "The current profiler step is stored in ``prof.step_num``.\n",
        "\n",
        "The following example shows how to use all of the concepts above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRZ_cuCg3P7K",
        "outputId": "147fb1de-c1f4-4852-defc-bb6bfaeb9f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::cudnn_convolution        27.78%       4.110ms        36.43%       5.389ms     134.725us       2.267ms        65.43%       2.267ms      56.675us            40  \n",
            "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     652.000us        18.82%     652.000us       9.588us            68  \n",
            "                                 aten::cudnn_batch_norm         9.31%       1.377ms        18.61%       2.753ms      68.825us     594.000us        17.14%     594.000us      14.850us            40  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     344.000us         9.93%     344.000us      11.467us            30  \n",
            "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     273.000us         7.88%     273.000us      34.125us             8  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     263.000us         7.59%     263.000us      32.875us             8  \n",
            "                                             aten::add_         5.60%     829.000us         8.56%       1.267ms      22.625us     258.000us         7.45%     258.000us       4.607us            56  \n",
            "            ampere_scudnn_128x64_relu_xregs_large_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     224.000us         6.46%     224.000us     112.000us             2  \n",
            "                                       aten::clamp_min_         2.04%     302.000us         3.66%     542.000us      15.941us     217.000us         6.26%     217.000us       6.382us            34  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     217.000us         6.26%     217.000us       6.382us            34  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 14.794ms\n",
            "Self CUDA time total: 3.465ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::cudnn_convolution        27.56%       4.113ms        36.64%       5.468ms     136.700us       2.270ms        65.40%       2.270ms      56.750us            40  \n",
            "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     653.000us        18.81%     653.000us       9.603us            68  \n",
            "                                 aten::cudnn_batch_norm         9.41%       1.404ms        18.49%       2.759ms      68.975us     595.000us        17.14%     595.000us      14.875us            40  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     345.000us         9.94%     345.000us      11.500us            30  \n",
            "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     274.000us         7.89%     274.000us      34.250us             8  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     264.000us         7.61%     264.000us      33.000us             8  \n",
            "                                             aten::add_         5.19%     775.000us         8.04%       1.200ms      21.429us     259.000us         7.46%     259.000us       4.625us            56  \n",
            "            ampere_scudnn_128x64_relu_xregs_large_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     224.000us         6.45%     224.000us     112.000us             2  \n",
            "                                       aten::clamp_min_         2.09%     312.000us         3.75%     559.000us      16.441us     217.000us         6.25%     217.000us       6.382us            34  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     217.000us         6.25%     217.000us       6.382us            34  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 14.922ms\n",
            "Self CUDA time total: 3.471ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This is called at the end of each cycle we specified\n",
        "# It's used to process a new trace by obtaining new table ourput OR saving output on disk as a trace file\n",
        "def trace_handler(p):\n",
        "    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n",
        "    print(output)\n",
        "    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n",
        "\n",
        "# Schedule to use\n",
        "schedule = torch.profiler.schedule(\n",
        "    wait=1,\n",
        "    warmup=1,\n",
        "    active=2\n",
        ")\n",
        "\n",
        "\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    schedule=schedule,\n",
        "    on_trace_ready=trace_handler) as p:\n",
        "\n",
        "    # Run an example for 8 steps\n",
        "    for idx in range(8):\n",
        "        model(inputs)\n",
        "\n",
        "        # profiler.step sends a signal to the profiler that next step has started\n",
        "        # Current step is stored as profile.step_num\n",
        "        p.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5s0rhMJ4XTl"
      },
      "source": [
        "What we see:\n",
        "* It only outputted 2 times, as the trace_handler is only called twice due to the schedule being used on only 8 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwBs36Ki4U84"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ldm')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "463c2b245ce7e6dc33c5f775f4732a1d4046139a314551395d57d6fe9844923f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
